{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPszubK+4GOr5q7EkPwpZB5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1_PB77SBXOoc"},"outputs":[],"source":["#1.Explain the concept of forward propagation in a neural network'"]},{"cell_type":"code","source":["Forward Propagation in a Neural Network\n","Forward Propagation is the process of passing input data through the layers of a neural network to obtain an output. Here’s a brief breakdown:\n","\n","Input Layer: The process starts with the input layer, where the input data is fed into the network.\n","\n","Hidden Layers: The input data moves through the hidden layers. Each neuron in a hidden layer takes the inputs, applies a weight to each, sums them up, and then applies an activation function to introduce non-linearity.\n","\n","Output Layer: The final layer (output layer) produces the network's prediction or output. The output can be in various forms, such as a probability distribution over classes (for classification) or a continuous value (for regression).\n","\n","Key Steps:\n","Weighted Sum: For each neuron, the weighted sum of the inputs is calculated.\n","\n","Activation Function: An activation function (like ReLU or Sigmoid) is applied to this weighted sum to produce the neuron's output.\n","\n","Propagation: This output is then passed as input to the next layer until the final output layer is reached.\n","\n","Forward propagation is essential for computing the output of a neural network, which can then be compared to the actual target values during the training process to compute the loss and update the network's weights through backpropagation."],"metadata":{"id":"3JieC3kBXZcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2.What is the purpose of the activation function in forward propagation\n"],"metadata":{"id":"mPT5W3L2XZQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Purpose of the Activation Function in Forward Propagation\n","The activation function introduces non-linearity into the neural network, allowing it to learn and model complex patterns in the data. Here are its main purposes:\n","\n","Non-Linearity: Without an activation function, the neural network would only be able to model linear relationships, limiting its ability to solve complex problems.\n","\n","Feature Learning: Activation functions enable the network to learn and extract intricate features from the input data by allowing the network to make complex transformations.\n","\n","Bounded Output: Some activation functions, like the sigmoid, squash the output to a specific range (e.g., between 0 and 1), making it suitable for probabilistic interpretations.\n","\n","In essence, activation functions are crucial for the network’s ability to understand and solve complex tasks beyond simple linear mappings."],"metadata":{"id":"_TAJM1mwXZH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3.Describe the steps involved in the backward propagation (backpropagation) algorithm"],"metadata":{"id":"8imWzjGhXZEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Backward Propagation (Backpropagation) Steps\n","Backpropagation is a key algorithm used for training neural networks. It involves updating the weights of the network to minimize the error between the predicted and actual outputs. Here's a concise overview of the steps:\n","\n","Forward Propagation:\n","\n","Pass the input data through the network to obtain the predicted output.\n","\n","Compute the loss (error) by comparing the predicted output with the actual target values.\n","\n","Backward Propagation:\n","\n","Compute Gradients:\n","\n","Calculate the gradient of the loss with respect to each weight in the network using the chain rule. This involves computing the partial derivatives of the loss with respect to the weights in each layer, starting from the output layer and moving backward through the hidden layers.\n","\n","Update Weights:\n","\n","Adjust the weights using the gradients computed in the previous step. This is typically done using an optimization algorithm like Gradient Descent, which updates the weights in the direction that reduces the loss.\n","\n","Repeat:\n","\n","Iterate the above steps (forward and backward propagation) for multiple epochs (iterations) until the network's performance improves and the loss converges to a minimum value."],"metadata":{"id":"Z--wBc-KXZBa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4.What is the purpose of the chain rule in backpropagation\n"],"metadata":{"id":"AmHAnVEEXY9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Purpose of the Chain Rule in Backpropagation\n","The chain rule is essential in backpropagation for the following reasons:\n","\n","Gradient Computation: It allows us to compute the gradient of the loss function with respect to each weight in the network. This gradient is necessary to update the weights in the direction that minimizes the loss.\n","\n","Layer-by-Layer Calculation: The chain rule enables the calculation of gradients layer by layer, from the output layer back to the input layer. This ensures that we can efficiently compute the impact of each weight on the overall loss.\n","\n","Handling Complex Networks: In neural networks with multiple layers, the chain rule helps in managing the complex dependencies between layers. It ensures that the error is properly propagated backward through all the layers.\n","\n","In essence, the chain rule is crucial for the backpropagation algorithm to update the network's weights effectively, thereby optimizing its performance."],"metadata":{"id":"_F9rPCX5XY6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#5.' Implement the forward propagation process for a simple neural network with one hidden layer using NumPy."],"metadata":{"id":"ozyKvpEWXY3N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Define the sigmoid activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Forward propagation function\n","def forward_propagation(X, W1, b1, W2, b2):\n","    # Input to hidden layer\n","    Z1 = np.dot(X, W1) + b1\n","    A1 = sigmoid(Z1)\n","\n","    # Hidden layer to output layer\n","    Z2 = np.dot(A1, W2) + b2\n","    A2 = sigmoid(Z2)\n","\n","    return A2\n","\n","# Example inputs and weights\n","X = np.array([[0.5, 0.2, 0.1]])  # Input data\n","W1 = np.array([[0.4, 0.2], [0.1, 0.3], [0.2, 0.5]])  # Weights for input to hidden layer\n","b1 = np.array([0.1, 0.2])  # Bias for hidden layer\n","W2 = np.array([[0.3], [0.7]])  # Weights for hidden to output layer\n","b2 = np.array([0.1])  # Bias for output layer\n","\n","# Perform forward propagation\n","output = forward_propagation(X, W1, b1, W2, b2)\n","print(\"Output:\", output)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGAeqo9wYmid","executionInfo":{"status":"ok","timestamp":1735206836848,"user_tz":-330,"elapsed":4054,"user":{"displayName":"shubhangi belkunde","userId":"13336077163669606462"}},"outputId":"1bb977e3-8eef-4e69-8ba6-fbc73499e289"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Output: [[0.66730447]]\n"]}]}]}